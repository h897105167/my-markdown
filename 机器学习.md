# 机器学习

## 1. 线性回归

数据：模型中能当做自变量的值

目标：模型预测的值

参数：各个数据的影响因子，以及生成的模型中的常量

举例：
$$
h_θ(X) = θ_{0} + θ_{1}X_{1} + θ_{2}X_{2}
$$
其中目标为h(X)，X1和X2为数据

$θ_1$和$θ_2$是$X_1$和$X_2$的参数，是核心影响

$θ_0$是偏置项，是模型完成后的微调

将该式转换为矩阵运算，可以得到
$$
h_θ(X) = θ^TX^{(i)}
$$
其中$X_1$恒等于1



### 1.1 误差和似然函数

对每个样本y，都有
$$
y^{(i)} = θ^TX^{(i)} + ε^{(i)} \tag1
$$
其中ε为预测值与真实值的偏差，误差也可以表示成
$$
ε^{(i)} = y^{(i)} -  θ^TX^{(i)} \tag2
$$


$ε^{(i)}$为独立同分布，均值为0，方差为$θ^2$的高斯分布

故误差的概率密度分布为
$$
p(ε^{(i)}) = \frac{1}{\sqrt{2π}σ}exp(-\frac{(ε^{(i)})^2}{2σ^2}) \tag3
$$
(2)代入(3)得
$$
p(y^{(i)}| x^{(i)};θ) = \frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)}| x^{(i)};θ)^2}{2σ^2}) \tag4
$$
似然函数$L(θ)$
$$
L(θ) = \prod_i^mp(y^{(i)}| x^{(i)};θ) = \prod_i^m\frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)} -  θ^TX^{(i)})^2}{2σ^2}) \tag5
$$
对数似然函数$logL(θ)$
$$
logL(θ) = \sum_i^mlog\frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)} -  θ^TX^{(i)})^2}{2σ^2}) \tag6
$$
展开化简得
$$
\sum_i^mlog\frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)} -  θ^TX^{(i)})^2}{2σ^2}) \\
 = m\,log\frac{1}{\sqrt{2π}σ}-\frac{1}{σ^2}\frac{1}{2}\sum_i^m(y^{(i)} -  θ^TX^{(i)})^2 \tag7
$$


目标函数$J(θ)$	
$$
J(θ) =\frac{1}{2}\sum_i^m(y^{(i)} -  θ^TX^{(i)})^2 \tag8
$$


求该函数的极小值点$θ_n$，就是线性回归的主要任务(最小二乘法)

对$J(θ)$求偏导得
$$
\nabla_θJ(θ) =\frac{1}{2}\sum_i^m(y^{(i)} -  θX^{(i)})^2 = \frac{1}{2}(Xθ-y)^T(Xθ-y)\\ 
= \nabla_θ(\frac{1}{2}(θ^TX^T-y^T)(Xθ-y)) \hfill\\
= \nabla_θ(\frac{1}{2}(θ^TX^TXθ-θ^TX^Ty - y^TXθ + y^Ty)\hfill\\
= \frac{1}{2}(2X^TXθ-X^Ty-(y^TX)^T = X^TXθ-X^Ty\hfill \tag9
$$
求解得
$$
θ = (X^TX)^{-1}X^Ty \tag{10}
$$

### 1.2 梯度下降

​	上述方法不一定可求解（矩阵不一定可逆），可以当做矩阵$X^TX$可逆时的一个特例。、

​	一般的求解方法为：给机器一堆数据，告诉机器怎样的学习方式是对的（目标函数），然后让它朝着这个方向去做。每次优化一点，一步步完成迭代。

​	对于线性回归问题，我们要对目标函数$J(θ)$寻找最小值。

​	具体步骤：

​		（1）找到最合适的方向（求梯度方向）

​		（2）走一小步

​		（3）按照方向与步伐更新参数与梯度

​	目标函数$J(θ)$：
$$
J(θ) = \frac{1}{2m}\sum_i^m(y^i-h_θ(x^i))^2 \tag1
$$
​	求梯度可得：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = -\frac{1}{m}\sum_i^m(y^i-h_\theta(x^i))x^i_j \tag2
$$

 1. 批量梯度下降（Batch Gradient Descent，BGD）

    以导函数的相反数作为梯度下降算法中的下降量，即
    $$
    \theta_j^{'} = \theta_j + \frac{1}{m}\sum_i^m(y^i-h_\theta(x^i))x^i_j \tag3
    $$
    缺点：迭代速度太慢

2. 随机梯度下降（Stochastic Gradient Descent，SGD）

   每次找同一个样本，迭代速度快
   $$
   \theta_j^{'} = \theta_j + (y^i-h_\theta(x^i))x^i_j \tag4
   $$

​	缺点：不一定朝着收敛的方向

3. 小批量梯度下降法(Mini-Batch Gradient Descent, MBGD)

   每次更新选择一部分数据来计算，选择的样本数称为batch size
   $$
   \theta_j: = \theta_j -\alpha\frac{1}{10}\sum_{k=i}^{i+9} (y^{(k)}-h_\theta(x^{(k)}))x^{(k)}_j \tag5
   $$
   常见的batch size数量为2的5次方，2的6次方

   batch size越大，结果越精确，但速度越慢，内存占用越大



### 1.3 学习率

​	minibatch中的$\alpha$，称为LR(learning rate)，一般设置为LR = 0.01 或 LR = 0.001

​	学习率过大，会导致学习过程中损失函数产生震荡、越过最优点、梯度爆炸，难以收敛

​	学习率过小，会导致学习速度太慢，效率太低

​	
